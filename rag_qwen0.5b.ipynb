{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG with Qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Clear any unused GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Step 0: Clear the Chroma database if it exists\n",
    "persist_dir = \"./chroma.db\"\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "# Step 1: Folder for JSON Files\n",
    "input_folder = \"Vietnam-Law-rag_json\"\n",
    "\n",
    "# Step 2: Load All JSON Files and Convert to LangChain Documents\n",
    "documents = []\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        base_file_name = os.path.splitext(file_name)[0]  # Remove the extension for `file_id`\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Convert JSON data to LangChain Document objects\n",
    "        documents.extend([\n",
    "            Document(\n",
    "                page_content=entry[\"text\"],\n",
    "                metadata={\n",
    "                    \"id\": entry[\"id\"],\n",
    "                    \"article\": entry[\"article\"],\n",
    "                    \"clause\": entry[\"clause\"],\n",
    "                    \"title\": entry[\"title\"],\n",
    "                    \"file_id\": base_file_name\n",
    "                }\n",
    "            )\n",
    "            for entry in data\n",
    "        ])\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {input_folder}.\")\n",
    "\n",
    "# Step 3: Initialize HuggingFace Embeddings\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Step 4: Create Chroma Vector Store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# Step 5: Load Qwen Model\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Step 6: Set Device for GPU/CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Step 7: Create a Text-Generation Pipeline with GPU/CPU and Handle Tokenization Warning\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    device=device,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Wrap the pipeline for LangChain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Step 8: Create a Retrieval-Based QA System\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Clean up GPU memory and move model to CPU after training/inference\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")  # Move model back to CPU to free up GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"System is ready for retrieval-based QA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query\n",
    "## RAG database builded on cloud servers, fetch them then run the below cell\n",
    "\n",
    "The aim is to optimize the returned data after the query search before push into the LLM Models, below here use Qwen2.5 for example.\n",
    "\n",
    "Just download the chroma.db, then symlink or put them in the current working git folder, then run the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Move model to GPU at the start\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Increase retrieval limit\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 250})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "def deduplicate_documents(documents):\n",
    "    \"\"\"\n",
    "    Remove duplicate content from the retrieved documents.\n",
    "    \"\"\"\n",
    "    seen_content = set()\n",
    "    unique_documents = []\n",
    "    for doc in documents:\n",
    "        # Use page content as the key for deduplication\n",
    "        content_key = doc.page_content.strip()\n",
    "        if content_key not in seen_content:\n",
    "            seen_content.add(content_key)\n",
    "            unique_documents.append(doc)\n",
    "    return unique_documents\n",
    "\n",
    "def expand_context_with_children(documents):\n",
    "    \"\"\"\n",
    "    Expand parent clauses by appending child clause content using regex to identify parent-child relationships.\n",
    "    \"\"\"\n",
    "    # Organize documents by parent id prefix\n",
    "    grouped_docs = {}\n",
    "    for doc in documents:\n",
    "        # Extract parent ID using regex to match parent prefix (e.g., \"Điều 2.33\")\n",
    "        match = re.match(r\"(.*?\\.\\d+)\", doc.metadata[\"id\"])\n",
    "        parent_id = match.group(1) if match else doc.metadata[\"id\"]\n",
    "        if parent_id not in grouped_docs:\n",
    "            grouped_docs[parent_id] = []\n",
    "        grouped_docs[parent_id].append(doc)\n",
    "\n",
    "    # Create expanded documents\n",
    "    expanded_documents = []\n",
    "    for parent_id, docs in grouped_docs.items():\n",
    "        # Sort documents to ensure children are added in order (e.g., \"Điều 2.33a\" comes after \"Điều 2.33\")\n",
    "        sorted_docs = sorted(docs, key=lambda d: d.metadata[\"id\"])\n",
    "        # Combine content from parent and children\n",
    "        combined_text = \" \".join(d.page_content for d in sorted_docs)\n",
    "        # Use the first document's metadata for the combined document\n",
    "        parent_doc = sorted_docs[0]\n",
    "        expanded_documents.append({\n",
    "            \"id\": parent_doc.metadata[\"id\"],\n",
    "            \"article\": parent_doc.metadata.get(\"article\"),\n",
    "            \"clause\": parent_doc.metadata.get(\"clause\"),\n",
    "            \"title\": parent_doc.metadata.get(\"title\"),\n",
    "            \"text\": combined_text,\n",
    "            \"file_id\": parent_doc.metadata.get(\"file_id\")\n",
    "        })\n",
    "\n",
    "    return expanded_documents\n",
    "\n",
    "# Query and retrieval\n",
    "query = \"An toàn lao động là gì\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Step 1: Deduplicate retrieved documents\n",
    "unique_docs = deduplicate_documents(result[\"source_documents\"])\n",
    "\n",
    "# Step 2: Expand context for parent clauses\n",
    "expanded_docs = expand_context_with_children(unique_docs)\n",
    "\n",
    "# Print the Result\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "\n",
    "# Print the Source Documents\n",
    "print(\"Source Documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "\n",
    "# Move model to CPU to release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
