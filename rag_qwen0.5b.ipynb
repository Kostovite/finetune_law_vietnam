{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG with Qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rag_json_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Step 2: Load All JSON Files and Convert to LangChain Documents\u001b[39;00m\n\u001b[1;32m     24\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_folder):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     27\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, file_name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rag_json_folder'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Clear any unused GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Step 0: Clear the Chroma database if it exists\n",
    "persist_dir = \"./chroma.db\"\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "# Step 1: Folder for JSON Files\n",
    "input_folder = \"Vietnam-Law-rag_json\"\n",
    "\n",
    "# Step 2: Load All JSON Files and Convert to LangChain Documents\n",
    "documents = []\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        base_file_name = os.path.splitext(file_name)[0]  # Remove the extension for `file_id`\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Convert JSON data to LangChain Document objects\n",
    "        documents.extend([\n",
    "            Document(\n",
    "                page_content=entry[\"text\"],\n",
    "                metadata={\n",
    "                    \"id\": entry[\"id\"],\n",
    "                    \"article\": entry[\"article\"],\n",
    "                    \"clause\": entry[\"clause\"],\n",
    "                    \"title\": entry[\"title\"],\n",
    "                    \"file_id\": base_file_name  # Add file name as an identifier\n",
    "                }\n",
    "            )\n",
    "            for entry in data\n",
    "        ])\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {input_folder}.\")\n",
    "\n",
    "# Step 3: Initialize HuggingFace Embeddings\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Step 4: Create Chroma Vector Store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "# Step 5: Load Qwen Model\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Step 6: Set Device for GPU/CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Step 7: Create a Text-Generation Pipeline with GPU/CPU and Handle Tokenization Warning\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    device=device,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Wrap the pipeline for LangChain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Step 8: Create a Retrieval-Based QA System\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Clean up GPU memory and move model to CPU after training/inference\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")  # Move model back to CPU to free up GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"System is ready for retrieval-based QA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2809/607843824.py:6: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Thuốc giả là thuốc được sản xuất thuộc một trong các trường hợp sau đây:\n",
      "\n",
      "Ưu tiên mua thuốc thuộc Danh mục sản phẩm quốc gia.\n",
      "\n",
      "Phối hợp với Bộ Y tế kiểm tra, thanh tra và xử phạt vi phạm pháp luật về giá thuốc.\n",
      "\n",
      "Chủ trì tổ chức thực hiện công tác phổ biến, giáo dục pháp luật về giá thuốc;\n",
      "\n",
      "Question: Thuốc giả là gì?\n",
      "Helpful Answer: Thuốc giả là thuốc được sản xuất thuộc một trong các trường hợp sau đây:\n",
      "Source Documents:\n",
      "Metadata: {'article': 'Điều 2', 'clause': '33', 'id': 'Điều 2.33', 'title': 'Giải thích từ ngữ'}\n",
      "Content: Thuốc giả là thuốc được sản xuất thuộc một trong các trường hợp sau đây:\n",
      "\n",
      "Metadata: {'article': 'Điều 7', 'clause': '4c', 'id': 'Điều 7.4c', 'title': 'Chính sách của Nhà nước về dược'}\n",
      "Content: Ưu tiên mua thuốc thuộc Danh mục sản phẩm quốc gia.\n",
      "\n",
      "Metadata: {'article': 'Điều 111', 'clause': '2', 'id': 'Điều 111.2', 'title': 'Trách nhiệm quản lý nhà nước về giá thuốc của Bộ Công Thương'}\n",
      "Content: Phối hợp với Bộ Y tế kiểm tra, thanh tra và xử phạt vi phạm pháp luật về giá thuốc.\n",
      "\n",
      "Metadata: {'article': 'Điều 109', 'clause': '3', 'id': 'Điều 109.3', 'title': 'Trách nhiệm quản lý nhà nước về giá thuốc của Bộ Y tế'}\n",
      "Content: Chủ trì tổ chức thực hiện công tác phổ biến, giáo dục pháp luật về giá thuốc;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Test the RAG System\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "query = \"Thuốc giả là gì?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the Result\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"Source Documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "\n",
    "# Move model to CPU to release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
