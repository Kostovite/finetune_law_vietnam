{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG with Qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Clear any unused GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Step 0: Clear the Chroma database if it exists\n",
    "persist_dir = \"./chroma.db\"\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "# Step 1: Load Your JSON File\n",
    "with open(\"rag_format.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Step 2: Convert JSON Data to LangChain Document Objects\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=entry[\"text\"],\n",
    "        metadata={\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"article\": entry[\"article\"],\n",
    "            \"clause\": entry[\"clause\"],\n",
    "            \"title\": entry[\"title\"]\n",
    "        }\n",
    "    )\n",
    "    for entry in data\n",
    "]\n",
    "\n",
    "# Step 3: Initialize HuggingFace Embeddings\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Step 4: Create Chroma Vector Store\n",
    "\n",
    "# Chroma may have some issues with the latest version of ChromaDB\n",
    "# pip install --upgrade chromadb==0.4.14\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=\"./chroma.db\"\n",
    ")\n",
    "\n",
    "# Step 5: Load Qwen Model\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Step 6: Set Device for GPU/CPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    if gpu_count > 1:\n",
    "        device = 0\n",
    "    else:\n",
    "        device = 0\n",
    "else:\n",
    "    device = -1\n",
    "\n",
    "# Step 7: Create a Text-Generation Pipeline with GPU/CPU and Handle Tokenization Warning\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    device=device,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Wrap the pipeline for LangChain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Step 8: Create a Retrieval-Based QA System\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Clean up GPU memory and move model to CPU after training/inference\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")  # Move model back to CPU to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2809/607843824.py:6: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Thuốc giả là thuốc được sản xuất thuộc một trong các trường hợp sau đây:\n",
      "\n",
      "Ưu tiên mua thuốc thuộc Danh mục sản phẩm quốc gia.\n",
      "\n",
      "Phối hợp với Bộ Y tế kiểm tra, thanh tra và xử phạt vi phạm pháp luật về giá thuốc.\n",
      "\n",
      "Chủ trì tổ chức thực hiện công tác phổ biến, giáo dục pháp luật về giá thuốc;\n",
      "\n",
      "Question: Thuốc giả là gì?\n",
      "Helpful Answer: Thuốc giả là thuốc được sản xuất thuộc một trong các trường hợp sau đây:\n",
      "Source Documents:\n",
      "Metadata: {'article': 'Điều 2', 'clause': '33', 'id': 'Điều 2.33', 'title': 'Giải thích từ ngữ'}\n",
      "Content: Thuốc giả là thuốc được sản xuất thuộc một trong các trường hợp sau đây:\n",
      "\n",
      "Metadata: {'article': 'Điều 7', 'clause': '4c', 'id': 'Điều 7.4c', 'title': 'Chính sách của Nhà nước về dược'}\n",
      "Content: Ưu tiên mua thuốc thuộc Danh mục sản phẩm quốc gia.\n",
      "\n",
      "Metadata: {'article': 'Điều 111', 'clause': '2', 'id': 'Điều 111.2', 'title': 'Trách nhiệm quản lý nhà nước về giá thuốc của Bộ Công Thương'}\n",
      "Content: Phối hợp với Bộ Y tế kiểm tra, thanh tra và xử phạt vi phạm pháp luật về giá thuốc.\n",
      "\n",
      "Metadata: {'article': 'Điều 109', 'clause': '3', 'id': 'Điều 109.3', 'title': 'Trách nhiệm quản lý nhà nước về giá thuốc của Bộ Y tế'}\n",
      "Content: Chủ trì tổ chức thực hiện công tác phổ biến, giáo dục pháp luật về giá thuốc;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Test the RAG System\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "query = \"Thuốc giả là gì?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the Result\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"Source Documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "\n",
    "# Move model to CPU to release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
