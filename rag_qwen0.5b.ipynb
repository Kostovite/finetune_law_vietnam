{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG with Qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Step 0: Clear the Chroma database if it exists\n",
    "persist_dir = \"./chroma.db\"\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "# Step 1: Folder for JSON Files\n",
    "input_folder = \"Vietnam-Law-rag_json\"\n",
    "\n",
    "# Step 2: Load All JSON Files and Convert to LangChain Documents\n",
    "documents = []\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        base_file_name = os.path.splitext(file_name)[0]  # Remove the extension for `file_id`\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Convert JSON data to LangChain Document objects\n",
    "        documents.extend([\n",
    "            Document(\n",
    "                page_content=entry[\"text\"],\n",
    "                metadata={\n",
    "                    \"id\": entry[\"id\"],\n",
    "                    \"article\": entry[\"article\"],\n",
    "                    \"clause\": entry[\"clause\"],\n",
    "                    \"title\": entry[\"title\"],\n",
    "                    \"file_id\": base_file_name\n",
    "                }\n",
    "            )\n",
    "            for entry in data\n",
    "        ])\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {input_folder}.\")\n",
    "\n",
    "# Step 3: Initialize HuggingFace Embeddings\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Step 4: Create Chroma Vector Store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "print(\"Chroma database created and saved at:\", persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query\n",
    "## RAG database builded on cloud servers, fetch them then run the below cell\n",
    "\n",
    "The aim is to optimize the returned data after the query search before push into the LLM Models, below here use Qwen2.5 for example.\n",
    "\n",
    "Just download the chroma.db, then symlink or put them in the current working git folder, then run the second cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma database loaded.\n",
      "Model and pipeline initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_chroma import Chroma  # Use the updated Chroma import\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Step 1: Load the Chroma Database\n",
    "persist_dir = \"./chroma.db\"\n",
    "\n",
    "# Initialize the embedding function\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Load the Chroma database with the embedding function\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_dir,\n",
    "    embedding_function=embeddings_model\n",
    ")\n",
    "\n",
    "print(\"Chroma database loaded.\")\n",
    "\n",
    "# Step 2: Load Qwen Model\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Step 3: Set Device for GPU/CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Step 4: Create a Text-Generation Pipeline with GPU/CPU\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    device=device,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Wrap the pipeline for LangChain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"Model and pipeline initialized.\")\n",
    "\n",
    "# Move model to CPU to release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'HuggingFaceEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7f806ee5cec0> search_kwargs={'k': 300}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Move model to GPU\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Increase retrieval limit\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 250})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Query and retrieval\n",
    "query = \"Thời hạn chuẩn bị xét xử phúc thẩm là bao lâu\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the Result\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "\n",
    "# Print the Source Documents\n",
    "print(\"Source Documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "\n",
    "# Move model to CPU to release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
