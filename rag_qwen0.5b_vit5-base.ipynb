{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG with Qwen2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# ViT5 Model Details\n",
    "model_name = \"VietAI/vit5-base\"  # Use the ViT5 model\n",
    "\n",
    "# Initialize ViT5 tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "class ViT5Embeddings:\n",
    "    def __init__(self, model, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def embed_text(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embeddings for a given text.\"\"\"\n",
    "        # Tokenize and get model outputs\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # For embedding, we only need the encoder output (not the decoder)\n",
    "        with torch.no_grad():  # Disable gradients to save memory during inference\n",
    "            outputs = self.model.encoder(**inputs)  # Use encoder directly\n",
    "\n",
    "        # Use mean pooling of the last hidden states\n",
    "        hidden_states = outputs.last_hidden_state  # Shape: [batch_size, seq_length, hidden_dim]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
    "        sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        mean_pooled = sum_embeddings / sum_mask\n",
    "        \n",
    "        return mean_pooled.squeeze(0).detach().cpu()\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[torch.Tensor]:\n",
    "        \"\"\"Generate embeddings for a batch of documents.\"\"\"\n",
    "        return [self.embed_text(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Generate an embedding for a query.\"\"\"\n",
    "        return self.embed_text(text)\n",
    "\n",
    "# Initialize ViT5 embeddings\n",
    "vit5_embeddings = ViT5Embeddings(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Step 1: Clear Chroma database if it exists\n",
    "persist_dir = \"./chroma.db\"\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "# Step 2: Load JSON files and convert to LangChain Documents\n",
    "input_folder = \"./finetune_law_vietnam/Vietnam-Law-rag_json\"\n",
    "documents = []\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        base_file_name = os.path.splitext(file_name)[0]\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        documents.extend([\n",
    "            Document(\n",
    "                page_content=entry[\"text\"],\n",
    "                metadata={\n",
    "                    \"id\": entry[\"id\"],\n",
    "                    \"article\": entry[\"article\"],\n",
    "                    \"clause\": entry[\"clause\"],\n",
    "                    \"title\": entry[\"title\"],\n",
    "                    \"file_id\": base_file_name\n",
    "                }\n",
    "            )\n",
    "            for entry in data\n",
    "        ])\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {input_folder}.\")\n",
    "\n",
    "# Step 3: Create embeddings for documents\n",
    "embedded_documents = [\n",
    "    {\n",
    "        \"embedding\": vit5_embeddings.embed_text(doc.page_content),\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"content\": doc.page_content,\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Step 4: Store in Chroma Vector Store\n",
    "vectorstore = Chroma.from_embeddings(\n",
    "    embeddings=[doc[\"embedding\"] for doc in embedded_documents],\n",
    "    documents=[doc[\"content\"] for doc in embedded_documents],\n",
    "    metadatas=[doc[\"metadata\"] for doc in embedded_documents],\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "vectorstore.persist()\n",
    "print(\"Chroma database created and saved at:\", persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query\n",
    "## RAG database builded on cloud servers, fetch them then run the below cell\n",
    "\n",
    "The aim is to optimize the returned data after the query search before push into the LLM Models, below here use Qwen2.5 for example.\n",
    "\n",
    "Just download the chroma.db, then symlink or put them in the current working git folder, then run the second cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27915/1845614436.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma database loaded.\n",
      "Model and pipeline initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModel\n",
    "\n",
    "# VietAI Embedding Class (for VietAI/vit5-base)\n",
    "class VietAIEmbeddings:\n",
    "    def __init__(self, model, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def embed_text(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embeddings for a given text.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
    "        sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        mean_pooled = sum_embeddings / sum_mask\n",
    "        return mean_pooled.squeeze(0).detach().cpu()\n",
    "\n",
    "    def embed_query(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Generate an embedding for a query.\"\"\"\n",
    "        return self.embed_text(text)\n",
    "\n",
    "# Step 1: Load the Chroma Database\n",
    "persist_dir = \"./chroma.db\"\n",
    "\n",
    "# Reinitialize VietAI/vit5-base tokenizer and model\n",
    "vit5_model_id = \"VietAI/vit5-base\"\n",
    "vit5_tokenizer = AutoTokenizer.from_pretrained(vit5_model_id)\n",
    "vit5_model = AutoModel.from_pretrained(vit5_model_id)\n",
    "vit5_embeddings = VietAIEmbeddings(model=vit5_model, tokenizer=vit5_tokenizer)\n",
    "\n",
    "# Load the Chroma database\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_dir,\n",
    "    embedding_function=vit5_embeddings.embed_query  # Use VietAI/vit5-base embedding function\n",
    ")\n",
    "\n",
    "print(\"Chroma database loaded.\")\n",
    "\n",
    "# Step 2: Load Qwen Model for Text Generation\n",
    "qwen_model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_model_id)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(qwen_model_id)\n",
    "\n",
    "# Step 3: Set Device for GPU/CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Step 4: Create a Text-Generation Pipeline with GPU/CPU for Qwen Model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=qwen_model,\n",
    "    tokenizer=qwen_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    device=device,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Wrap the pipeline for LangChain\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"Qwen Model and pipeline initialized.\")\n",
    "\n",
    "# Step 5: Move Qwen Model to CPU to release GPU memory (if used)\n",
    "torch.cuda.empty_cache()\n",
    "qwen_model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Move model to GPU\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Increase retrieval limit\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Query and retrieval\n",
    "query = \"Trẻ em là gì\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the Result\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "\n",
    "# Print the Source Documents\n",
    "print(\"Source Documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "\n",
    "# Move model to CPU to release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELL 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "\n",
    "# Move model to GPU\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Increase retrieval limit\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 250})\n",
    "\n",
    "# Define a custom retriever function that intercepts and modifies the results\n",
    "def custom_retriever(query, k=300):\n",
    "    # Retrieve relevant documents from the retriever\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    modified_documents = []\n",
    "    \n",
    "    for doc in results:\n",
    "        # Example of cleaning content: Removing unwanted words or patterns (adjust regex as needed)\n",
    "        cleaned_content = re.sub(r\"unwanted_pattern\", \"\", doc.page_content)\n",
    "        \n",
    "        # Ensure that the content is relevant to the query (optional: further filtering logic)\n",
    "        if 'thời hạn' in cleaned_content.lower():  # Assuming query context involves time limits like 'thời hạn'\n",
    "            modified_doc = {\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"page_content\": cleaned_content,\n",
    "            }\n",
    "            modified_documents.append(modified_doc)\n",
    "    \n",
    "    return modified_documents\n",
    "\n",
    "# Define the instruction for the model\n",
    "instruction = \"Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "\n",
    "# Use the custom retriever to retrieve and modify documents\n",
    "query = \"Trẻ em là gì\"\n",
    "retrieved_docs = custom_retriever(query)\n",
    "\n",
    "# Debug: Check if any documents are retrieved and modified\n",
    "if not retrieved_docs:\n",
    "    print(\"No relevant documents retrieved or filtered out.\")\n",
    "else:\n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
    "\n",
    "# Concatenate the instruction with the retrieved documents\n",
    "input_to_llm = instruction + \" \" + \" \".join([doc[\"page_content\"] for doc in retrieved_docs])\n",
    "\n",
    "# Check the input format before passing to the model\n",
    "print(\"Input to LLM:\", input_to_llm[:500])  # Print the first 500 characters for debugging\n",
    "\n",
    "# Send the modified input to your LLM (e.g., Qwen2.5 model)\n",
    "result_from_llm = hf(input_to_llm)  # Assuming 'hf' is your model callable\n",
    "print(\"LLM Output:\", result_from_llm)\n",
    "\n",
    "# Move model to CPU to release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
